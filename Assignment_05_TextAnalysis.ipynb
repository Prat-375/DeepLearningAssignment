{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqjhbAO7Vtoi"
      },
      "source": [
        "# importing tensorflow and numpy\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq6PG_nlZ1md"
      },
      "source": [
        "Get and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojUh-cSQV3sN"
      },
      "source": [
        "## Run once in cobab to retrieve king james bible input file\n",
        "path_to_file = \"the-king-james-bible.txt\"\n",
        "text = open(path_to_file,'r').read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L431Eo28V9FM",
        "outputId": "0228ac2f-e454-46ed-fc9e-98a1af8e2861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "char_to_ind = {char:ind for ind,char in enumerate(vocab)}\n",
        "ind_to_char = np.array(vocab)\n",
        "encoded_text = np.array([char_to_ind[c] for c in text])\n",
        "print(char_to_ind)\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, '*': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '?': 22, 'A': 23, 'B': 24, 'C': 25, 'D': 26, 'E': 27, 'F': 28, 'G': 29, 'H': 30, 'I': 31, 'J': 32, 'K': 33, 'L': 34, 'M': 35, 'N': 36, 'O': 37, 'P': 38, 'Q': 39, 'R': 40, 'S': 41, 'T': 42, 'U': 43, 'V': 44, 'W': 45, 'Y': 46, 'Z': 47, 'a': 48, 'b': 49, 'c': 50, 'd': 51, 'e': 52, 'f': 53, 'g': 54, 'h': 55, 'i': 56, 'j': 57, 'k': 58, 'l': 59, 'm': 60, 'n': 61, 'o': 62, 'p': 63, 'q': 64, 'r': 65, 's': 66, 't': 67, 'u': 68, 'v': 69, 'w': 70, 'x': 71, 'y': 72, 'z': 73, '\\ufeff': 74}\n",
            "75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_KFeZjoSwxJ"
      },
      "source": [
        "seq_len = 120\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "sequences = char_dataset.batch(seq_len+1,drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDM72cvCUjBJ"
      },
      "source": [
        "def create_seq_targets(seq):\n",
        "  input_text = seq[:-1]\n",
        "  target_text = seq[1:]\n",
        "  return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHffnASaU25Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "908c8a2b-c861-494c-874e-47286c33e4af"
      },
      "source": [
        "dataset = sequences.map(create_seq_targets)\n",
        "\n",
        "for input_txt, target_txt in dataset.take(1):\n",
        "  print(input_txt.numpy())\n",
        "  print(\" \".join(ind_to_char[input_txt.numpy()]))\n",
        "  print('\\n')\n",
        "  print(target_txt.numpy())\n",
        "  print(\" \".join(ind_to_char[target_txt.numpy()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[74 42 55 52  1 28 56 65 66 67  1 24 62 62 58  1 62 53  1 35 62 66 52 66\n",
            " 20  1  1 25 48 59 59 52 51  1 29 52 61 52 66 56 66  0  0  0 11 20 11  1\n",
            " 31 61  1 67 55 52  1 49 52 54 56 61 61 56 61 54  1 29 62 51  1 50 65 52\n",
            " 48 67 52 51  1 67 55 52  1 55 52 48 69 52 61  1 48 61 51  1 67 55 52  1\n",
            " 52 48 65 67 55  9  0  0 11 20 12  1 23 61 51  1 67 55 52  1 52 48 65 67]\n",
            "ï»¿ T h e   F i r s t   B o o k   o f   M o s e s :     C a l l e d   G e n e s i s \n",
            " \n",
            " \n",
            " 1 : 1   I n   t h e   b e g i n n i n g   G o d   c r e a t e d   t h e   h e a v e n   a n d   t h e   e a r t h . \n",
            " \n",
            " 1 : 2   A n d   t h e   e a r t\n",
            "\n",
            "\n",
            "[42 55 52  1 28 56 65 66 67  1 24 62 62 58  1 62 53  1 35 62 66 52 66 20\n",
            "  1  1 25 48 59 59 52 51  1 29 52 61 52 66 56 66  0  0  0 11 20 11  1 31\n",
            " 61  1 67 55 52  1 49 52 54 56 61 61 56 61 54  1 29 62 51  1 50 65 52 48\n",
            " 67 52 51  1 67 55 52  1 55 52 48 69 52 61  1 48 61 51  1 67 55 52  1 52\n",
            " 48 65 67 55  9  0  0 11 20 12  1 23 61 51  1 67 55 52  1 52 48 65 67 55]\n",
            "T h e   F i r s t   B o o k   o f   M o s e s :     C a l l e d   G e n e s i s \n",
            " \n",
            " \n",
            " 1 : 1   I n   t h e   b e g i n n i n g   G o d   c r e a t e d   t h e   h e a v e n   a n d   t h e   e a r t h . \n",
            " \n",
            " 1 : 2   A n d   t h e   e a r t h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSmWN8R3XC3Z"
      },
      "source": [
        "batch_size = 128\n",
        "buffer_size = 10000\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJuNILbDXpQx",
        "outputId": "f98fc98c-4b70-4d32-d45d-9fa4c03c257c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FRg9OblYBgl"
      },
      "source": [
        "# number of neurons in the hidden layer\n",
        "vocab_size = len(vocab)\n",
        "rnn_neurons = 1026\n",
        "embed_dim = 64\n",
        "\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,GRU,Dense\n",
        "\n",
        "def sparse_cat_loss(y_true,y_pred):\n",
        "  return sparse_categorical_crossentropy(y_true,y_pred,from_logits=True)\n",
        "\n",
        "def create_model(vocab_size,embed_dim,rnn_neurons,batch_size):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size,embed_dim,batch_input_shape=[batch_size,None]))\n",
        "  model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
        "  model.add(Dense(vocab_size))\n",
        "  model.compile('adam',loss=sparse_cat_loss)\n",
        "  return model\n",
        "\n",
        "model = create_model(vocab_size=vocab_size,embed_dim=embed_dim,rnn_neurons=rnn_neurons,batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkvof4uOI15r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "27c3dceb-18eb-4ab6-8f72-cf221e9f00b2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (128, None, 64)           4800      \n",
            "_________________________________________________________________\n",
            "gru_9 (GRU)                  (128, None, 1026)         3361176   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (128, None, 75)           77025     \n",
            "=================================================================\n",
            "Total params: 3,443,001\n",
            "Trainable params: 3,443,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXpmfDV5I_Nt"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwqqQpNIa3Lp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "07caa6a4-4871-4af8-9850-13e6f7decf3c"
      },
      "source": [
        "example_batch_predictions.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([128, 120, 75])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T08SDbQZa_kd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "e7ca9561-1040-4a66-b8c3-26b05e77a9b8"
      },
      "source": [
        "epochs = 10\n",
        "model.fit(dataset,epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "279/279 [==============================] - 67s 242ms/step - loss: 2.4898\n",
            "Epoch 2/10\n",
            "279/279 [==============================] - 68s 245ms/step - loss: 1.6464\n",
            "Epoch 3/10\n",
            "279/279 [==============================] - 68s 244ms/step - loss: 1.3397\n",
            "Epoch 4/10\n",
            "279/279 [==============================] - 68s 244ms/step - loss: 1.2062\n",
            "Epoch 5/10\n",
            "279/279 [==============================] - 68s 244ms/step - loss: 1.1376\n",
            "Epoch 6/10\n",
            "279/279 [==============================] - 68s 243ms/step - loss: 1.0942\n",
            "Epoch 7/10\n",
            "279/279 [==============================] - 68s 244ms/step - loss: 1.0616\n",
            "Epoch 8/10\n",
            "279/279 [==============================] - 68s 244ms/step - loss: 1.0372\n",
            "Epoch 9/10\n",
            "279/279 [==============================] - 68s 244ms/step - loss: 1.0148\n",
            "Epoch 10/10\n",
            "279/279 [==============================] - 68s 243ms/step - loss: 0.9958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faa38043358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWVJHwPIz6AE"
      },
      "source": [
        "model.save('king_james.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PakFCMN0nmC"
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDvNLNY41LSt"
      },
      "source": [
        "model = create_model(vocab_size,embed_dim,rnn_neurons,batch_size=1)\n",
        "model.load_weights('king_james.h5')\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBmoL6rt1yH0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "d6107cfa-38c9-443a-e741-9758db655f6d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (1, None, 64)             4800      \n",
            "_________________________________________________________________\n",
            "gru_10 (GRU)                 (1, None, 1026)           3361176   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (1, None, 75)             77025     \n",
            "=================================================================\n",
            "Total params: 3,443,001\n",
            "Trainable params: 3,443,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU_0F-zWbijy"
      },
      "source": [
        "def generate_text(model,start_seed,gen_size=500,temp=1.0):\n",
        "  num_generate = gen_size\n",
        "  input_eval = [char_to_ind[s] for s in start_seed]\n",
        "  input_eval = tf.expand_dims(input_eval,0)\n",
        "  text_generated = []\n",
        "  temperature = temp\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions,0)\n",
        "    predictions = predictions/temperature\n",
        "    predicted_id = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
        "    input_eval = tf.expand_dims([predicted_id],0)\n",
        "    text_generated.append(ind_to_char[predicted_id])\n",
        "  return (start_seed+\"\".join(text_generated))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHqBxBGgeOsj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "943e64da-9be9-4b0c-8ef9-aa9aaba51280"
      },
      "source": [
        "print(generate_text(model,\"Genesis\",gen_size=1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Genesise at the countra, and pain fraiket, till I break in two of God.\n",
            "\n",
            "1:24 And for the devils pour out anithers, that great Ahinah the son of Jerahor\n",
            "the Balhathites.\n",
            "\n",
            "26:29 Neither came neither stil.\n",
            "\n",
            "1:14 Seeing Noah went heal, and set him up.\n",
            "\n",
            "1:7 And when he gave him like a ray rumbur of your lips. And he went\n",
            "down unto me appused in mine enemies' fear: 1:27  Where is afraid at his\n",
            "decree, and fastened the face of the\n",
            "anointing accospitaling of the eliquetiles, and did they not pray you,\n",
            "even with our fathers obtainly me about: they of the land, yet not shew\n",
            "them by vineyard of the children, and pronounce hundred pleasing up on the other.\n",
            "\n",
            "41:11 And in nothing cometh ene affer likengs out of the land, seek thy land\n",
            "that weight of your words; Hear the word is the Lord:20 And a certain woman, beholding the thigh the hith his\n",
            "princes, and their a thousand, and a ram, and upright\n",
            "in their loins, and are thine eyes, that I might receive a gloriou also was power to\n",
            "hear God, to day of judgmen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQwNXFnrgQpx"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}